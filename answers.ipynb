{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "\n",
    "In this homework, you'll review some linear algebra, and practice using `sklearn` and `scipy.optimize` to solve several variations of linear models.  You'll also use Pandas and Scikit learn for a simple machine learning task on a UCI data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "\n",
    "A standard problem in statistics to solve the multivariate linear regression problem.\n",
    "\\begin{equation}\n",
    "y = X * \\beta + \\epsilon\n",
    "\\end{equation}\n",
    "The above notation is standard in statistics, but we'll use Householder notation by replacing $\\beta$ with `b`\n",
    "```\n",
    "y = X * b + eps\n",
    "```\n",
    "`X` is known as the [design matrix](https://en.wikipedia.org/wiki/Design_matrix), and consists of `n` rows of observations, each of which has `p` features.  `y` is a vector of `n` responses.  `b` is an unknown vector of `p` coefficients which we would like to find.  `eps` (epsilon) is a vector of length `n` with random noise, typically i.i.d. normally distributed with variance `sig` (sigma).\n",
    "\n",
    "In numpy notation, we could express this as\n",
    "```python\n",
    "y[i] = np.dot(X[i], b) + sig * np.random.randn()\n",
    "```\n",
    "\n",
    "We want to determine `b`, so that when me make a new observation `X[n]` we can predict the response `y[n]`.  Our goal is to minimize the mean square error\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathop{\\mathsf{minimize}}_b \\mathbb{E}((X[n]*b - y[n])^2)\n",
    "\\end{equation}\n",
    "\n",
    "The solution to this is the solution to the least squares problem\n",
    "\\begin{equation}\n",
    "\\mathop{\\mathsf{minimize}}_b \\frac{1}{n} \\|X*b - y\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "Where $n$ is the number of rows in $X$.  We'll let the solution to the problem be denoted $\\hat{b}$, or `bhat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0 - Linear Regression (30 points)\n",
    "\n",
    "There are a variety of ways to solve the above problem.  We'll assume that $n > p$, meaning there are more observations than features.\n",
    "\n",
    "### Generating Data\n",
    "\n",
    "Write a function `gen_lstsq(n, p, sig=0.1)` which will generate a linear least squares problem (return `X`, `y`, and `b` as described above).  `b` is the \"ground truth\" coefficients that we are looking for.  You can generate the matrix `X` using `np.random.randn`, and generate `b` using `np.random.randn` as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR factorization\n",
    "\n",
    "If we form a QR factorization $X = QR$, we can find $\\hat{b} = R^{-1} Q^T y$.\n",
    "\n",
    "Write a function `solve_lstsq_qr(X, y)` which estimates `b` using the QR factorization as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations\n",
    "\n",
    "Often, this is the way statistics textbooks solve the problem: $\\hat{b} = (X^T X)^{-1} X^T y$.  This is based on the normal equation $X^T X \\hat{b} = X^T y$.\n",
    "\n",
    "Write a function `solve_lstsq_normal(X, y)` which estimates `b` using the normal equations above.  You probably don't want to form the inverse - use an LU or Cholesky factorization instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Your Work\n",
    "\n",
    "Generate a few random problems to test that `solve_lstsq_qr` and `solve_lstsq_normal` give the same prediction $\\hat{b}$ (measure $\\|\\hat{b}_{qr} - \\hat{b}_{normal}\\|_2$ and check it is smaller than `1e-4`).  Use $n > p$.  Check against `solve_lstsq` in numpy or scipy as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the Mean Squared Error\n",
    "\n",
    "Write a function `mse` which estimates the mean squared error (MSE) $\\mathbb{E}[\\|X * \\hat{b} - y\\|_2^2]$.\n",
    "\n",
    "The function should have the call signature `mse(b, bhat, N=100, sig=0.1)`  You can generate `X` to be `N x p` using `np.random.randn`, and set `y = X * b + sig * np.random.randn(p)` (use the same value of `sig` that you use when generating the data).  You can then estimate the MSE by computing $\\frac{1}{N}\\|X * \\hat{b} - y\\|_2^2$\n",
    "\n",
    "Create a plot of the MSE vs the noise parameter `sig` (use the same value of `sig` when generating data and computing the MSE).  Put `sig` on a logartihmic axis ranging from `1e-4` to `10`.  Use `n=100`, `p=50` when generating data, and `N=100` when computing the MSE.  Put your plot on log-log axes.  Give it a title and axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 1\n",
    "\n",
    "Which of `solve_lstsq_qr` and `solve_lstsq_normal` is faster?  Give a justification for your answer (e.g. in terms of what you know about time to compute matrix multiplication and factorizations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_your discussion here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Solve the minimization problem \n",
    "\\begin{equation}\n",
    "\\mathop{\\mathsf{minimize}}_b \\frac{1}{n}\\|X*b - y\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "using `scipy.optimize.minimize`.  Wrap this in a function `solve_lstsq_opt(X, y)`.  Note that you can define the objective function inside `solve_lstsq_opt`\n",
    "\n",
    "**Jacobian**: you should implement the Jacobian of the objective function to use in the optimization problem.  Here's a derivation:  We'd like to minimize the objective function\n",
    "\\begin{equation}\n",
    "n f(b) = \\|X*b - y\\|_2^2 = (Xb - y)^T (Xb - y) = b^T X^T X b - 2 y^T X b + y^T y\n",
    "\\end{equation}\n",
    "\n",
    "We might write the above expression as\n",
    "\\begin{equation}\n",
    "n f(b) \\sum_{i,j} b_i (X^T X)_{i,j} b_j - 2\\sum_{j,i} y_i X_{i,j} b_j + y^T y\n",
    "\\end{equation}\n",
    "\n",
    "We can take a derivative with respect to $b_j$\n",
    "\\begin{equation}\n",
    "n \\frac{\\partial f}{\\partial b_j} = \\sum_{i\\ne j} b_i (X^T X)_{i,j} + \\sum_{i\\ne j} (X^T X)_{j,i} b_i + 2 (X^T X)_{j,j} b_j  - 2\\sum_{i} y_i X_{i,j}\n",
    "\\end{equation}\n",
    "\n",
    "Putting this in matrix form, we obtain\n",
    "\\begin{equation}\n",
    "J_f(b) =  \\frac{1}{n}\\big( b^T (X^T X) + b^T (X^T X)^T - 2y^T X\\big) = \\frac{2}{n} b^T (X^T X) -\\frac{2}{n}y^T X\n",
    "\\end{equation}\n",
    "\n",
    "So we can write $J_f(b) = \\frac{2}{n} b^T (X^T X) -\\frac{2}{n} y^T X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Your Work\n",
    "\n",
    "Generate a few random problems to test that `solve_lstsq_opt` agrees with `solve_lstsq_qr` and `solve_lstsq_normal` in part A.  Use the same tolerance for checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2\n",
    "\n",
    "How fast is `solve_lstsq_opt` compared to the functions you wrote in part A?  Give some justification for what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_your discussion - you can add more code/markdown cells_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn\n",
    "\n",
    "Use [`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) to solve the linear regression problem. You can get the vector `bhat` from `model.coef_`\n",
    "\n",
    "Make a plot for the MSE like you did above using `LinearRegression` instead of a `solve_lstsq` function.  Set the keyword `fit_intercept=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Ridge Regression (15 points)\n",
    "\n",
    "We'll now turn to the problem of what to do when `n < p` (there are fewer observations than features).  In this case we can solve $X * b = y$ exactly, but there are many possible values of $b$ which can satisfy the equation.\n",
    "\n",
    "The normal equations are singular, and the $R$ term of the QR factorization is not invertible in this case.  We need to do something else.\n",
    "\n",
    "Ridge regression seeks to solve the following optimization problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathop{\\mathsf{minimize}}_b \\frac{1}{n}\\|X*b - y\\|_2^2 + \\lambda \\|b\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "$\\lambda$ is a parameter you can choose to \"regularize\" the problem which you can choose.\n",
    "\n",
    "### Optimization\n",
    "\n",
    "Use `scipy.optimize.minimize` to minimize the above optimization problem.  Wrap this in a function `solve_ridge_opt(X, y, lam=0.1)` (`lam` should be provided as a keyword argument for the variable $\\lambda$).\n",
    "\n",
    "What is the Jacobian for the objective function for the minimization problem?  Incorporate this into your optimization problem.  Hint: differentiation is linear, so you just need to add a term to the Jacobian in Problem 0 based on the Jacobian of\n",
    "\\begin{equation}\n",
    "\\lambda \\|b\\|_2^2 = \\lambda b^T b = \\lambda \\sum_i b_i^2\n",
    "\\end{equation}\n",
    "\n",
    "---\n",
    "\n",
    "_put the expression for the Jacobian here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the MSE\n",
    "\n",
    "Set `n = 50`, `p=100`, and `sig=0.1`.  Make a plot that displays the MSE of `bhat` computed using `solve_ridge_opt` as `lam` varies between `1e-4` and `1e2`.  Use a `semilogx` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn\n",
    "\n",
    "Use [`sklearn.linear_model.Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) to solve the Ridge regression problem (note that this uses the keyword `alpha` where we have used `lam`). You can get the vector `bhat` from `model.coef_`\n",
    "\n",
    "Make a plot for the MSE like you did above using `Ridge` instead of your `solve_ridge_opt` function.  In addition to the keyword `alpha`, set `fit_intercept=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Lasso (15 points)\n",
    "\n",
    "The Lasso is L1-regularized regression.  This is often used when `p > n`, and when the parameter vector `b` is assumed to be sparse, meaning that it has few non-zero entries.\n",
    "\n",
    "The minimization problem is\n",
    "\\begin{equation}\n",
    "\\mathop{\\mathsf{minimize}}_b \\frac{1}{n} \\|X * b - y\\|_2^2 + \\lambda \\|b\\|_1\n",
    "\\end{equation}\n",
    "\n",
    "Where again, $\\lambda$ can be chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data\n",
    "\n",
    "We need to modify our generation of data to produce sparse `b`.  Write a function `gen_lstsq_sparse(n, p, sig=0.1, k=10)`, which generates `X` and `y` in the same way as `gen_lstsq`, but now `b` is generated to be a vector of length `p` with `k` random entries set to 1, and all other entries set to 0. Hint: look at `np.random.choice` for generating `k` random integers without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Recall we want to find `bhat` to solve\n",
    "\\begin{equation}\n",
    "\\mathop{\\mathsf{minimize}}_b \\frac{1}{n} \\|X * b - y\\|_2^2 + \\lambda \\|b\\|_1\n",
    "\\end{equation}\n",
    "\n",
    "Use `scipy.optimize.minimize` to minimize the above optimization problem.  Wrap this in a function `solve_lasso_opt(X, y, lam=0.1)` (`lam` should be provided as a keyword argument for the variable $\\lambda$).\n",
    "\n",
    "What is the Jacobian for the objective function for the minimization problem?  Incorporate this into your optimization problem.  Hint: differentiation is linear, so you just need to add a term to the Jacobian in Problem 0 based on the Jacobian of\n",
    "\\begin{equation}\n",
    "\\lambda \\|b\\|_1 = \\lambda \\sum_i |b_i|\n",
    "\\end{equation}\n",
    "\n",
    "If $b_i$ is zero, then take $\\partial_i |b_i| = 0$ (this is in the [subgradient](https://en.wikipedia.org/wiki/Subderivative)). Hint 2: look at `np.sign` for implementation\n",
    "\n",
    "---\n",
    "\n",
    "_put the expression for the Jacobian here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the MSE\n",
    "\n",
    "Set `n = 50`, `p=100`,`sig=0.1`, and `k=10` to generate a problem using `gen_lstsq_sparse`.  Make a plot that displays the MSE of `bhat` computed using `solve_lasso_opt` as `lam` varies between `1e-4` and `1e2`.  Use a `semilogx` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn\n",
    "\n",
    "Use [`sklearn.linear_model.Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) to solve the Lasso regression problem (note that this uses the keyword `alpha` where we have used `lam`). You can get the vector `bhat` from `model.coef_`\n",
    "\n",
    "Make a plot for the MSE like you did above using `Lasso` instead of your `solve_lasso_opt` function.  In addition to the keyword `alpha`, set `fit_intercept=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - Elastic Net (20 points)\n",
    "\n",
    "Another option when `p > n` is the [Elastic Net]() which combines L1 and L2 regularization.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathop{\\mathsf{minimize}}_b \\frac{1}{n} \\|X * b - y\\|_2^2 + \\lambda_1 \\|b\\|_1 + \\lambda_2 \\|b\\|_2^2\n",
    "\\end{equation}\n",
    "Where we get to choose $\\lambda_1$ and $\\lambda_2$.\n",
    "\n",
    "You can find a function that does this in [`sklearn.linear_model.ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet).\n",
    "\n",
    "In this problem, you'll write a class that emulates the behavior of a Scikit learn class. Use `scipy.optimize.minimize` to solve the optimization problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian\n",
    "\n",
    "Give an expression for the Jacobian for the optimization objective function.  You've already done all the hard work for this in problems 1 and 2\n",
    "\n",
    "---\n",
    "\n",
    "_put the Jacobian expression here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the Class\n",
    "\n",
    "Define a Python Class `ElasticNet`.  When you initialize the class you should be able to provide keyword arguments `lam1` for $\\lambda_1$ and `lam2` for $\\lambda_2$.  Give each these keyword arguments a default value of `0.5`.\n",
    "\n",
    "In addition to `__init__`, your class should provide two methods:\n",
    "1. `fit(X, y)` - sets an attribute `bhat` which solves the optimization problem (use the parameters `lam1` and `lam2` provided in initialization).  Use `scipy.optimize.minimize`\n",
    "2. `predict(X)` - predict values of `y` as `X @ bhat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the MSE\n",
    "\n",
    "Set `n=50`, `p=100`,`sig=0.1`, and `k=10` to generate a problem using `gen_lstsq_sparse`.  \n",
    "\n",
    "Set `n=50`, `p=100`, and `sig=0.1` to generate a problem using `gen_lstsq`.\n",
    "\n",
    "Make a plot that displays the MSE of the model computed using your `ElasticNet` class using `lam1=lam2` varying between `1e-4` and `1e2` for each of the generated problems (your plot should have 2 lines).  Give your plot a title, axis labels, and a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 - Real Data (20 points)\n",
    "\n",
    "In this problem you'll use the [Wine Quality Data Set](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/).  We'll focus on the red wines found in [`winequality-red.csv`](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv).  The goal is to train a machine learning model that will predict the wine quality from a variety of other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data set\n",
    "\n",
    "Load the wines in `windquality-red.csv` into a Pandas data frame. Note that this file has column labels, which are interpreted in the data set documentation:\n",
    "\n",
    "Input variables (based on physicochemical tests):\n",
    "1. fixed acidity\n",
    "2. volatile acidity\n",
    "3. citric acid\n",
    "4. residual sugar\n",
    "5. chlorides\n",
    "6. free sulfur dioxide\n",
    "7. total sulfur dioxide\n",
    "8. density\n",
    "9. pH\n",
    "10. sulphates\n",
    "11. alcohol\n",
    "\n",
    "Output variable (based on sensory data):\n",
    "\n",
    "12. quality (score between 0 and 10)\n",
    "\n",
    "Note that this file uses semi-colons `;` to delimit values.  You can use `delimiter=';'` as a keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 11 columns will be features for the model (used in the design matrix `X`), and the last column (quality) is the quantity we're tyring to predict (the response `y`).\n",
    "\n",
    "### Split the Data in to Training and Testing Sets\n",
    "\n",
    "Split the data `X, y` into `X_train, y_train, X_test, y_test` using `sklearn`.  Use 70% of the data for training, and 30% of the data for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Model and Measure Error\n",
    "\n",
    "Use [`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) to fit a linear regression model to your training data.\n",
    "\n",
    "Use [`sklearn.metrics.mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) to compute the MSE on your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Some Different Models\n",
    "\n",
    "Note: you don't need to know anything about these models.  You can just use the `fit` and `predict` methods, and the default parameters.\n",
    "\n",
    "Use the following `sklearn` classes to fit models and make predictions:\n",
    "* [`sklearn.ensemble.RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)\n",
    "* [`sklearn.neural_network.MLPRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)\n",
    "* [`sklearn.neighbors.KNeighborsRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor)\n",
    "\n",
    "Which model gives the best MSE on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pycourse)",
   "language": "python",
   "name": "pycourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
